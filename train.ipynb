{"cells":[{"cell_type":"markdown","source":["## **Setup and Imports**\n","\n","\n"],"metadata":{"id":"s6_zQIsDVBoi"},"id":"s6_zQIsDVBoi"},{"cell_type":"code","execution_count":3,"id":"B5477WKpAb_t","metadata":{"id":"B5477WKpAb_t","executionInfo":{"status":"ok","timestamp":1761667961233,"user_tz":420,"elapsed":6405,"user":{"displayName":"Narjes Nourzad","userId":"06581798672267077411"}}},"outputs":[],"source":["# Import necessary libraries\n","import pickle\n","import os\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","plt.rcParams['figure.figsize'] = (7,4)\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping"]},{"cell_type":"markdown","source":["## **Mount Google Drive and Load Data**"],"metadata":{"id":"snE2-iFhVLHT"},"id":"snE2-iFhVLHT"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMty79cx_-dG","outputId":"99f555cd-46eb-41ee-9f71-478c84cace8b","executionInfo":{"status":"ok","timestamp":1761667944741,"user_tz":420,"elapsed":21106,"user":{"displayName":"Narjes Nourzad","userId":"06581798672267077411"}}},"id":"aMty79cx_-dG","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["os.chdir('/content/drive/My Drive/Colab Notebooks')\n","\n","from google.colab import files\n","files.upload()\n","\n","[f for f in os.listdir() if 'coin' in f.lower()] # Just to check you have coins.csv and noisy_coins.csv in the right location!"],"metadata":{"id":"AtVZDrClB4lz"},"id":"AtVZDrClB4lz","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Dataset Visualization**"],"metadata":{"id":"HU_utcnLU78f"},"id":"HU_utcnLU78f"},{"cell_type":"code","execution_count":null,"id":"8j8zUWphAiXS","metadata":{"id":"8j8zUWphAiXS"},"outputs":[],"source":["# Load the dataset 'coins.csv' using pandas' read_csv function.\n","# Make sure coins.csv is in the directory where you're using Jupyter\n","# If using Google Colab, upload the coins file to the directory by usinf upload in the toolbar on the left\n","coins = pd.read_csv('coins.csv')\n","coins.tail(5)"]},{"cell_type":"code","source":["# Visualize the data distribution using a scatterplot\n","sns.scatterplot(x=\"reflectance\", y=\"weight\", data=coins, hue=\"denomination\")\n","plt.title(\"Scatterplot of Coin Reflectance and Weight\")\n","plt.show()"],"metadata":{"id":"_rKnu4_Im8tW"},"id":"_rKnu4_Im8tW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Data Preparation**"],"metadata":{"id":"Rv8FrJs1nwMj"},"id":"Rv8FrJs1nwMj"},{"cell_type":"code","execution_count":8,"id":"Xihkq4n3AwhB","metadata":{"id":"Xihkq4n3AwhB","collapsed":true,"executionInfo":{"status":"ok","timestamp":1761668009358,"user_tz":420,"elapsed":8,"user":{"displayName":"Narjes Nourzad","userId":"06581798672267077411"}}},"outputs":[],"source":["# [STUDENT SECTION: Prepare the data for training and testing]\n","\n","# Prepare the input features (X)\n","X = coins[[\"reflectance\", \"weight\"]].to_numpy()\n","\n","# Convert the denomination values (1 and 2) to binary labels (0 and 1) for binary classification.\n","# Hint: Subtract 1 from the denomination values to convert them to 0 and 1.\n","y = # Change this\n","\n","# Split the data into training and testing sets using train_test_split.\n","# Use an 80/20 split for training and testing and random_state of 42.\n","X_train, X_test, y_train, y_test = train_test_split() # Set the arguments\n","\n","# Normalize the input test and train features using StandardScaler to standardize reflectance and weight values.\n","scaler = StandardScaler()\n","# Use the scaler's fit_transform and transform methods to standardize the training and testing features.\n","#  (See https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe)\n","\n"]},{"cell_type":"markdown","source":["## **Build and Train the Model**"],"metadata":{"id":"IplnjnyloDpr"},"id":"IplnjnyloDpr"},{"cell_type":"code","execution_count":10,"id":"YhiFLmG9A1EG","metadata":{"id":"YhiFLmG9A1EG","executionInfo":{"status":"ok","timestamp":1761668015643,"user_tz":420,"elapsed":115,"user":{"displayName":"Narjes Nourzad","userId":"06581798672267077411"}}},"outputs":[],"source":["# [STUDENT SECTION: Define and compile the model]\n","# Instantiate and Train the Model\n","# - Build a neural network model using TensorFlow/Keras.\n","# - The model should have:\n","#    - An input layer with 2 input features (reflectance and weight).\n","#    - 1 or 2 hidden layers with the number of neurons at your discretion (use 'relu' activation).\n","#      (Experiment to find the best accuracy)\n","#    - An output layer with 1 neuron and 'sigmoid' activation for binary classification.\n","\n","\n","# - Compile the model using Adam optimizer with a learning rate of 0.001,\n","#   binary_crossentropy as the loss function, and 'accuracy' as the metric.\n","model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","#the above has nice accuracy\n","\n","\n","# - Train the model using X_train and y_train.\n","#    - Set validation_split to 0.2.\n","#    - Use EarlyStopping with patience=5 to prevent overfitting.\n","#    - Train the model for 100 epochs and use a batch size of 32.\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n","\n","# call the appropriate function to train the model\n","\n"]},{"cell_type":"markdown","source":["## **Evaluate the Model**"],"metadata":{"id":"3GfVyPU3ow_4"},"id":"3GfVyPU3ow_4"},{"cell_type":"markdown","id":"ul-aEjVGKs-F","metadata":{"id":"ul-aEjVGKs-F"},"source":["**experiement with the accuracy if used only 1 middle layer vs. 2 and so on**"]},{"cell_type":"code","execution_count":null,"id":"E40g64bOA6Y_","metadata":{"id":"E40g64bOA6Y_"},"outputs":[],"source":["# [STUDENT SECTION: Evaluate the model]\n","# Hint:\n","# - Call model.evaluate() on the test set (X_test, y_test) and print the test accuracy.\n"]},{"cell_type":"code","execution_count":null,"id":"lS5QJfu3A-Z4","metadata":{"id":"lS5QJfu3A-Z4"},"outputs":[],"source":["# Plot the Decision Boundary:\n","# - Define a function to plot the decision boundary of the trained model. This function should take in the feature matrix X, the labels y, and the trained model.\n","# - This function should:\n","#    - Generate a mesh grid over the feature space (reflectance and weight).\n","#    - Use the model to predict labels for each point in the mesh grid.\n","#    - Plot the decision boundary using contourf.\n","#    - Overlay the training data points using seaborn's scatterplot.\n","def plot_decision_boundary(X, y, model):\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n","    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = np.round(Z).reshape(xx.shape)\n","    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.coolwarm)\n","    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y.flatten(), palette='bright', edgecolor=\"k\")\n","    plt.xlabel('Reflectance')\n","    plt.ylabel('Weight')\n","    plt.title('Decision Boundary of the Binary Classifier')\n","    plt.show()\n","\n","\n","# [STUDENT SECTION: Call the function to plot the decision boundary by passing the correct arguments]\n","plot_decision_boundary()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"WacbNOMU88ye","metadata":{"id":"WacbNOMU88ye"},"outputs":[],"source":["# Save the Trained Model:\n","# - Save the trained model to a file named 'model.h5' using model.save().\n","\n","# [STUDENT SECTION: Save the model]\n","\n","# Load the Saved Model and Use it to evalue the \"test\" data:\n","# - Load the model using keras.models.load_model and re-evaluate it on the test data.\n"]},{"cell_type":"code","execution_count":null,"id":"eNMx7pSK_-TZ","metadata":{"id":"eNMx7pSK_-TZ"},"outputs":[],"source":["# [STUDENT SECTION: Evaluate the loaded model on the test data]\n","# the accuracy of the loaded model on the test data.  Examine the output\n","# of predict and think about how to compute the accuracy of the predictions using\n","# the y_test data.  Hint:  You may need to round the predictions to 0 or 1 using np.round()\n","# When you use model.predict() in a binary classification problem, the output is usually\n","# a 2D array where each element is a list containing a single predicted probability\n","# (e.g., [[0.1], [0.9], [0.3], ...]). To compare these predictions to your 1D y_test array\n","# (e.g., [0, 1, 0, ...]), you need to \"flatten()\" the 2D array into a 1D array.\n","\n","\n","# Print the accuracy of the loaded model on the test data\n","\n"]},{"cell_type":"markdown","source":["### **Eval on noisy data**"],"metadata":{"id":"Yl71wURnqArz"},"id":"Yl71wURnqArz"},{"cell_type":"code","source":["\n","# In this section, you’ll test how well your trained model performs when the data isn't perfectly clean.\n","#  - The goal is to see whether your model can still correctly classify coins when there’s noise (for example, small measurement errors or variations in reflectance/weight).\n","noisy_coins = pd.read_csv('noisy_coins.csv')\n","\n","\n","# Visualize the data distribution using a scatterplot\n","\n","\n"],"metadata":{"id":"6Fm3ptsOqO4G","executionInfo":{"status":"ok","timestamp":1761669098016,"user_tz":420,"elapsed":9,"user":{"displayName":"Narjes Nourzad","userId":"06581798672267077411"}}},"id":"6Fm3ptsOqO4G","execution_count":19,"outputs":[]},{"cell_type":"code","source":["#Prepare features and labels\n","# - Extract the same features as before: reflectance and weight.\n","# - Convert the denomination column (1/2) into binary labels (0/1).\n","# - IMPORTANT: Use the SAME StandardScaler (scaler) that you used for training.\n","#   This ensures both datasets are normalized in the same way.\n","\n","#Predict and evaluate\n","# - The model will output probabilities between 0 and 1.\n","# - Convert these probabilities into binary predictions using a threshold of 0.5."],"metadata":{"id":"JRH3k3FkrD2Z"},"id":"JRH3k3FkrD2Z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate and visualize confusion matrix\n","# - Add the needed libraries\n","# - The confusion matrix shows how many samples were correctly or incorrectly classified.\n","# - Rows correspond to TRUE labels; columns correspond to PREDICTED labels.\n","\n","\n","# Print precision, recall, and F1-score"],"metadata":{"id":"mP05cVFDp0wm","executionInfo":{"status":"ok","timestamp":1761677811046,"user_tz":420,"elapsed":2,"user":{"displayName":"Narjes Nourzad","userId":"06581798672267077411"}}},"id":"mP05cVFDp0wm","execution_count":20,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"muuIzpM3Lhc_"},"id":"muuIzpM3Lhc_","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":5}